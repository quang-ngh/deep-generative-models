{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZSXSjGdpXvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import math\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "# !pip install tqdm\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bObGvKVQqQZD",
        "outputId": "8a545c31-6aea-4f99-fc07-d58402a8e873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-5mEl04pXvi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ0pDv82pXvi"
      },
      "outputs": [],
      "source": [
        "def save_and_display(grid: torch.Tensor, nrow: int, path: str) -> None:\n",
        "\n",
        "    grid = make_grid(grid, nrow = nrow)\n",
        "    save_image(grid, path)\n",
        "    return grid, path\n",
        "\n",
        "def save_gif(list_of_grids: list, path: str) -> None:\n",
        "\n",
        "   toImg = transforms.ToPILImage()\n",
        "\n",
        "   #  Each image is [4, c, h, w]\n",
        "   images = [make_grid(x, nrow=4) for x in list_of_grids]\n",
        "   gif_src = [np.array(toImg(img)) for img in images]\n",
        "\n",
        "   imageio.mimsave(path, gif_src, format = 'GIF', fps = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK0u2ahGpXvj"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OybpqMiCpXvj"
      },
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R-BBsg5pXvk"
      },
      "source": [
        "## U-Net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IALZwm8pXvk"
      },
      "outputs": [],
      "source": [
        "#   Sinoisudal Embedding\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "\n",
        "        super(TimeEmbedding, self).__init__()\n",
        "        self.dim = embed_dim\n",
        "\n",
        "    def forward(self, t: torch.Tensor):\n",
        "\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device = device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat([emb.sin(), emb.cos()], dim = -1)\n",
        "\n",
        "        # assert t.is_cuda == True, \"Not match device\"\n",
        "        return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbBueDIpXvk"
      },
      "outputs": [],
      "source": [
        "#  Residual Block\n",
        "class ResBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel: int, out_channel: int, num_groups: int, dropout_rate: float, down_up: str):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.swish = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels = in_channel, out_channels = out_channel, kernel_size = (3,3), padding = 'same', stride = 1, device=device)\n",
        "        self.conv2 = nn.Conv2d(in_channels = out_channel, out_channels = out_channel, kernel_size = (5,5), padding = 'same', stride = 1, device= device)\n",
        "        self.res_conv = nn.Conv2d(in_channels = in_channel, out_channels=out_channel, kernel_size=(7,7), padding = 'same', stride = 1, device=device)\n",
        "\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=num_groups, num_channels = in_channel, device = device)\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=num_groups, num_channels = out_channel, device = device)\n",
        "        self.mode = down_up\n",
        "        if down_up == \"down\":\n",
        "            self.time_embedding = TimeEmbedding(out_channel)\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                nn.Linear(out_channel, out_channel, device=device),\n",
        "                nn.SiLU(),\n",
        "            )\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        \"\"\"\n",
        "            X -> GroupNorm1 -> Swish -> Conv1 -> Out1\n",
        "            Time embedding -> Swish -> MLP -> Out2\n",
        "            (Out1 + Out2) -> GroupNorm2 -> Swish -> Dropout -> Conv2 -> Skip connection\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        out = x\n",
        "\n",
        "        # X -> GroupNorm -> Swish -> Conv1 -> Out1\n",
        "        out = self.group_norm1(out)\n",
        "        out = self.swish(out)\n",
        "        out = self.conv1(out)\n",
        "        assert (H,W) == (out.shape[2], out.shape[3]), \"Not compatible shape\"\n",
        "\n",
        "        # Time Embedding in the case of downblock\n",
        "        if self.mode == \"down\":\n",
        "            time_embed = self.time_embedding(t)\n",
        "            time_embed = self.time_mlp(time_embed)\n",
        "            time_embed = time_embed.view(B, time_embed.shape[1], 1, 1)\n",
        "            out += time_embed\n",
        "            \n",
        "        #   Last\n",
        "        out = self.group_norm2(out)\n",
        "        out = self.swish(out)\n",
        "        out = self.conv2(out)\n",
        "        out += self.res_conv(x)\n",
        "\n",
        "        # assert out.get_device() == device, \"Not match devices\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLwfWDyIpXvl"
      },
      "outputs": [],
      "source": [
        "#   Downsample Block\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels = out_channel, kernel_size=(3,3), padding = 'same', stride = 1, device=device)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=(4,4), padding = (1,1), stride = 2, device=device)\n",
        "        self.norm = nn.BatchNorm2d(out_channel, device = device)\n",
        "    \n",
        "    def forward(self, x, t = None):\n",
        "        B,C,H,W = x.shape    \n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm(out)\n",
        "        out = self.activation(out)\n",
        "        spatial_size = out.shape\n",
        "        assert (spatial_size[2], spatial_size[3]) == (H//2, W//2), \"Not compatible size!\"\n",
        "        return out\n",
        "\n",
        "#   Upsample Block\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(UpBlock, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels = out_channel, out_channels = out_channel, kernel_size=(4,4), padding =1, stride = 2, device=device)\n",
        "        self.norm = nn.BatchNorm2d(out_channel, device = device)\n",
        "        \n",
        "    def forward(self, x, t = None):\n",
        "        B,C,H,W = x.shape\n",
        "        out = self.deconv(x)\n",
        "        out = self.norm(out)\n",
        "        # assert out.get_device() == device, \"Not match devices\"\n",
        "        assert (out.shape[2], out.shape[3]) == (H*2, W*2), \"Size is not compatible!\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PGug2g2pXvm"
      },
      "outputs": [],
      "source": [
        "class VisualAttention(nn.Module):\n",
        "    def __init__(self, num_heads, in_channel, out_channel):\n",
        "        super(VisualAttention, self).__init__()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrNGDSQhpXvm"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block:\n",
        "        4 resolutions (256, 128, 64, 32)\n",
        "        4 blocks per resolutions. In and out channels will be defined as \n",
        "        256 x 256: (3,6) -> (6,6) -> (6,6) -> (6,6)\n",
        "        128 x 128: (6,12) -> (12,12) -> (12,12) -> (12,12)\n",
        "        64 x 64: (12,15) -> (15,15) -> (15,15) -> (15,15)\n",
        "        32 x 32: (15,18) -> (18,18) -> (18, 18) -> (18,18)\n",
        "    \n",
        "    Downsample Block:\n",
        "        256 -> 128 -> 64 -> 32\n",
        "        (6,6) -> (12,12) -> (15,15) -> (18,18)\n",
        "    Upsample Block:\n",
        "        32 -> 64 -> 128 -> 256\n",
        "        (18, 15) -> (15,12) -> (12,6) -> (6,3)\n",
        "    \"\"\"\n",
        "    def __init__(self, resolutions:list, in_channels: list, out_channels: list, num_groups: int, image_size: tuple):\n",
        "        super(Unet, self).__init__()\n",
        "        \n",
        "        assert len(resolutions) == len(in_channels), \"Given {} resolutions in down sampling but just have {} expected in channels\".format(len(resolutions), len(in_channels))\n",
        "        self.down_residual = nn.ModuleList([nn.ModuleList([]) for idx in range(len(resolutions))])\n",
        "        self.midsample = nn.ModuleList([])\n",
        "        self.upsample = nn.ModuleList([nn.ModuleList([]) for idx in range(len(resolutions))])\n",
        "        self.downsample = nn.ModuleList([])\n",
        "        self.downsample_res = nn.ModuleList([])\n",
        "        \n",
        "        in_out = list(zip(in_channels, out_channels))\n",
        "        # Setting for the down-sample and upsample blocks\n",
        "        for idx, pair_res in enumerate(in_out):\n",
        "            \n",
        "            in_channel, out_channel = pair_res\n",
        "            for resblock_idx in range(4):\n",
        "                in_channel_down = in_channel if resblock_idx == 0 else out_channel\n",
        "                out_channel_down = out_channel\n",
        "                self.down_residual[idx].append(\n",
        "                    ResBlock(in_channel_down, out_channel_down, num_groups, 0.0, down_up = \"down\")\n",
        "                )\n",
        "                \n",
        "                in_channel_up = out_channel * 2 if resblock_idx == 0 else in_channel\n",
        "                out_channel_up = in_channel\n",
        "                self.upsample[idx].append(\n",
        "                    ResBlock(in_channel_up, out_channel_up, num_groups, 0.0, down_up = \"up\")\n",
        "                )\n",
        "            self.downsample.append(\n",
        "                DownBlock(out_channel_down, out_channel_down)\n",
        "            )\n",
        "            self.downsample_res.append(\n",
        "                DownBlock(out_channel_down, out_channel_down)\n",
        "            )\n",
        "            self.upsample[idx].append(\n",
        "                UpBlock(out_channel_up, out_channel_up)\n",
        "            )\n",
        "        self.upsample = self.upsample[::-1]\n",
        "        \n",
        "        self.midsample.append(nn.Conv2d(out_channels[-1], out_channels[-1], kernel_size=(3,3), padding = 'same', stride = 1, device = device))\n",
        "        self.midsample.append(nn.Identity())\n",
        "        self.out_channels = out_channels\n",
        "        self.in_channels = in_channels\n",
        "        self.resolutions = resolutions\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        \n",
        "        \"\"\"\n",
        "        ResBlock (n blocks)\n",
        "            :input: x: batch of images, t: time steps\n",
        "        DownBlock:\n",
        "            :input: Output of ResBlock\n",
        "            256 x 256 -> 32 x 32\n",
        "        MiddleBlock:\n",
        "            :input: Output of Downblock -> Size / 8\n",
        "            32 x 32 -> 16 x 16\n",
        "        UpBlock:\n",
        "            :input: Output of MiddleBlock\n",
        "            16 x 16 -> 256 x 256\n",
        "        \"\"\"\n",
        "        B,C,H,W = x.shape\n",
        "        out = x\n",
        "        n_resolutions = len(self.resolutions)\n",
        "        resolution_down = []\n",
        "        residual_down = []\n",
        "        for down_idx, downblock in enumerate(self.down_residual):\n",
        "            \n",
        "            # Pass through the residual blocks and down sample \n",
        "            for dblock in downblock:\n",
        "                out = dblock(out, t)\n",
        "            out = self.downsample[down_idx](out, t)\n",
        "            dB, dC, dH, dW = out.shape\n",
        "            scale_down_factor = 2 ** (down_idx + 1)\n",
        "            assert (dB, dC, dH, dW) == (B, self.out_channels[down_idx], H //scale_down_factor, W // scale_down_factor),\\\n",
        "            \"Shape is not compatible. Expected outshape of {} but {}\".format((B, self.out_channels[down_idx], H //scale_down_factor, W // scale_down_factor), (dB, dC, dH, dW))\n",
        "            resolution_down.append(out)\n",
        "\n",
        "        # Mid sample block: 16 x 16 -> 16 x 16\n",
        "        for mid_block in self.midsample:\n",
        "            out = mid_block(out)\n",
        "            \n",
        "        # Passing through the residual blocks and up sample\n",
        "        for up_idx, upblock in enumerate(self.upsample):\n",
        "            out = torch.cat((out, resolution_down.pop()), dim = 1)\n",
        "            for ublock in upblock:\n",
        "                out = ublock(out, t)\n",
        "            uB, uC, uH, uW = out.shape\n",
        "            # assert (uB, uC, uH, uW) == (B, self.in_channels[::-1][up_idx], self.resolutions[n_resolutions - up_idx - 1], self.resolutions[n_resolutions - up_idx - 1]),\\\n",
        "            # \"Shape is not compatible. Expected outshape of {} but {}\".format((B, self.in_channels[::-1][up_idx], self.resolutions[n_resolutions - up_idx - 1], self.resolutions[n_resolutions - up_idx - 1]), (uB, uC, uH, uW))\n",
        "        \n",
        "        oB, oC, oH, oW = out.shape\n",
        "        assert (oB, oC, oH, oW) == (B, C, H, W), \"Output shape is not compatible. Expect {} but {}\".format((B,C,H,W), (oB, oC, oH, oW))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUEvcRB7pXvn"
      },
      "source": [
        "## Diffusion Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "podmVs0QpXvn"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, beta_start, beta_end, time_steps, sampling_steps, network):\n",
        "        super(Diffusion, self).__init__()\n",
        "        \n",
        "        self.beta = torch.linspace(beta_start, beta_end, time_steps, device=device, requires_grad=False)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.cum_prod_alpha = torch.cumprod(self.alpha, dim = 0)\n",
        "        self.one_minus_cumprod = 1.0 - self.cum_prod_alpha\n",
        "        self.denoise_net = network\n",
        "        self.sampling_steps = sampling_steps\n",
        "        self.time_steps = time_steps\n",
        "        \n",
        "    def _posterior_sample(self, x, t):\n",
        "        batch, c, h, w = x.shape\n",
        "        cumprod_t = self.cum_prod_alpha[t].view(batch, 1, 1, 1)\n",
        "        one_minus_cumprod_t = self.one_minus_cumprod[t].view(batch, 1, 1, 1)\n",
        "\n",
        "        noise = torch.randn_like(x, device = device, requires_grad=False)\n",
        "        std = torch.sqrt(one_minus_cumprod_t)\n",
        "        mean = torch.sqrt(cumprod_t) * x\n",
        "\n",
        "        return mean + std*noise, noise\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def _reverse(self, noise, t):\n",
        "        \n",
        "        B, C, H, W= noise.shape\n",
        "        z = torch.randn_like(noise) if t >= 1 else 0\n",
        "\n",
        "        time = torch.ones(B, dtype=torch.int64, device=device)*t\n",
        "\n",
        "        eps_theta = self.denoise_net(noise, time)\n",
        "        eps_coff = (1.0-self.alpha[t]) / ((1-self.cum_prod_alpha[t])**0.5)\n",
        "\n",
        "        x_previous = (1.0 / (self.alpha[t] ** 0.5)) * (noise - eps_coff * eps_theta) + z * ((1-self.alpha[t])**0.5)\n",
        "\n",
        "        return x_previous\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def sampling(self, image_shape: list, batch: int):\n",
        "        \n",
        "        C,H,W = image_shape\n",
        "        image = torch.randn(batch, C, H, W, device = device, requires_grad=False)\n",
        "        tracks = [image]\n",
        "\n",
        "        t = self.sampling_steps - 1\n",
        "        \n",
        "        while t >= 0:\n",
        "            image = self._reverse(image, t) #   Sample x_{t-1} from p(x_t-1|x_t)\n",
        "            tracks.append(image)\n",
        "            t-=1\n",
        "        \n",
        "        return image, tracks\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        out, noise = self._posterior_sample(x, t)    # Diffuse data\n",
        "        out = self.denoise_net(out, t)               # Predict noise\n",
        "        B,C,H,W = x.shape\n",
        "        oB, oC, oH, oW = out.shape\n",
        "        assert (B,C,H,W) == (oB, oC, oH, oW), \"Output shape is not compatible with input shape. Expect {} but {}\".format((B,C,H,W),(oB, oC, oH, oW) )\n",
        "        return out, noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp67Nh6wEq0I"
      },
      "outputs": [],
      "source": [
        "in_c = [3,12,18]\n",
        "out_c = [12,18,24]\n",
        "resolutions = [32,16,8]\n",
        "unet = Unet(resolutions, in_c, out_c, 3, (32,32)).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnBKc8u1pXvo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s8yzeF-pXvo"
      },
      "outputs": [],
      "source": [
        "def visualize(model, batch, save_path):\n",
        "\n",
        "    gen_img, tracks = model.sampling([3,32,32], batch)\n",
        "    grid = torchvision.utils.make_grid(gen_img, nrow=2)\n",
        "    torchvision.utils.save_image(grid, save_path[0])\n",
        "    # tracks.extend([gen_img]*100)\n",
        "    # save_gif(tracks, save_path[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5ycfur-pXvo"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, epochs, lr, save_per_epochs, clip_val, loss_type, save_path = None):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.denoise_net.parameters(), lr=lr)\n",
        "    print(optimizer)\n",
        "    print(\"Starting training...\")\n",
        "    \n",
        "    losses = []\n",
        "    if loss_type == \"l1\":\n",
        "      loss_fn = torch.nn.L1Loss(reduction = 'mean')\n",
        "    elif loss_type == \"l2\":\n",
        "      loss_fn = torch.nn.MSELoss(reduction = 'mean')\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        for (idx, dataset) in tqdm(enumerate(data_loader)):\n",
        "\n",
        "            x_0 = dataset[0].to(device)\n",
        "            t = (model.time_steps - 1) * torch.rand((x_0.shape[0],), device = device) + 1\n",
        "            t = t.long()\n",
        "            optimizer.zero_grad()\n",
        "            eps_theta, eps = model(x_0, t)\n",
        "\n",
        "            loss = loss_fn(eps_theta, eps)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.denoise_net.parameters(), clip_val)\n",
        "            optimizer.step()        \n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(\"Loss after {} = {}\".format(epoch, loss.item()))\n",
        "        if epoch % save_per_epochs == 0:\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Diffusion-Model/ckpts/ckpt_\"+str(epoch))\n",
        "            visualize(model, 4, [\"/content/drive/MyDrive/Diffusion-Model/gif/diff_\"+str(epoch)+\".png\"] )\n",
        "            print('Saved!')\n",
        "\n",
        "    print(\"End training!\")\n",
        "    plt.plot(losses)\n",
        "    plt.savefig(\"/content/drive/MyDrive/Diffusion-Model/ckpts/loss.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnkGTC_upXvo"
      },
      "outputs": [],
      "source": [
        "time_steps = 1000\n",
        "sampling_steps = 1000\n",
        "beta_start = 2e-4\n",
        "beta_end = 2e-1\n",
        "def get_model():\n",
        "\n",
        "    model = Diffusion(beta_start, beta_end, time_steps, sampling_steps, unet)\n",
        "    return models\n",
        "\n",
        "def get_model_pretrain(ckpt_path):\n",
        "    model = Diffusion(beta_start, beta_end, time_steps, sampling_steps, unet)\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    return model\n",
        "\n",
        "# ckpt_path = \"/content/drive/MyDrive/Diffusion-Model/ckpts/ckpt_240\"\n",
        "# model = get_model_pretrain(ckpt_path)\n",
        "model = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__66SqiJN4LK"
      },
      "outputs": [],
      "source": [
        "epochs = 500000\n",
        "lr = 0.001\n",
        "clip_val = 1.0\n",
        "save_per_epochs = 10\n",
        "loss_type = 'l1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND2VcpyDpXvo"
      },
      "outputs": [],
      "source": [
        "SIZE = 32\n",
        "batch_size = 100\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((SIZE, SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda( lambda t: (t * 2) - 1)\n",
        "])\n",
        "dataset = torchvision.datasets.CIFAR10(root = \".\", transform = transform, download = True)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cqqSIDgpXvo"
      },
      "outputs": [],
      "source": [
        "train(model, dataloader, epochs, lr, save_per_epochs, clip_val, loss_type)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}